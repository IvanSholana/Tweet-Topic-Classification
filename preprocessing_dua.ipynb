{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Not scanning for jupyter notebooks.\n",
      "INFO: Successfully saved requirements file in .\\requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!pipreqs ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean number, tanda baca\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean username\n",
    "def clean_username(text):\n",
    "    return re.sub(r'[@#]\\S+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean RT, RE[], and Non Alphabetic\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\bRT\\b', '', text)  # Remove 'RT'\n",
    "    text = re.sub(r'\\[RE [^\\]]+\\]', '', text)  # Remove '[RE xxxx]'\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated_chars(text):\n",
    "    exception_word = ['uu','pp'] \n",
    "    # Regular expression to remove repeated characters, except specified exception word\n",
    "    pattern = rf'(?<!\\b{exception_word})((.)\\2+)(?!\\2)'\n",
    "    return re.sub(pattern, r'\\2', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_formal(df, slang_to_formal, column):\n",
    "    # List untuk menyimpan hasil teks yang telah diformalisasi\n",
    "    formal_tokens = []\n",
    "    \n",
    "    # Iterasi melalui setiap baris dalam DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        temp = []\n",
    "        # Iterasi melalui setiap kata dalam kolom 'clean'\n",
    "        for word in row[column].split():\n",
    "            # Mengganti kata slang dengan kata formal jika ada dalam kamus, jika tidak, tetap menggunakan kata aslinya\n",
    "            temp.append(slang_to_formal.get(word, word))\n",
    "        # Menggabungkan kata-kata menjadi sebuah kalimat\n",
    "        res = \" \".join(temp)\n",
    "        formal_tokens.append(res)\n",
    "    \n",
    "    return formal_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_repeated_words(text):\n",
    "    # List kata yang akan dikecualikan dari pemisahan\n",
    "    exceptions = [\n",
    "    'tata', 'sisi', 'uu', 'wowo', 'cucu', 'papa', 'mama', 'wkwk', 'susu',\n",
    "    'bebe', 'kuku', 'nana', 'lele', 'dodo', 'pepe', 'lala', 'gugu', 'meme',\n",
    "    'kaka', 'tutu', 'pipi', 'wawa', 'cici', 'hihi', 'tete', 'mimi', 'yaya',\n",
    "    'popo', 'zaza', 'riri', 'fufu', 'bobo', 'sisis', 'pupa', 'kekek', 'lulu',\n",
    "    'toto', 'nene', 'yuyu', 'bababa', 'cicic', 'didi', 'papapa', 'mamama',\n",
    "    'tototo', 'kiki', 'wuwu', 'sissis', 'lololo', 'tutu', 'mimi', 'wawa',\n",
    "    'hoho', 'lol', 'gaga', 'tata', 'kekeke', 'dudu', 'fifi', 'didi', 'zizi',\n",
    "    'nini', 'mumu', 'wuwu', 'babi', 'jiji', 'wawa', 'mumu', 'hihi', 'titi',\n",
    "    'kiki', 'popo', 'pepe', 'zaza', 'nene', 'sissi', 'tati', 'tati', 'dedede',\n",
    "    'lulu', 'mumu', 'yuyu', 'riri', 'zizi', 'cucu', 'babi', 'mama', 'pupa',\n",
    "    'pupu', 'tete', 'hihi', 'kaka', 'wawa', 'dudu', 'gaga', 'pepe', 'mama',\n",
    "    'kukuku','pp'\n",
    "]\n",
    "    \n",
    "    # Regular expression to find repeated words, except specified exceptions\n",
    "    pattern = r'\\b(?!(?:' + '|'.join(exceptions) + r')\\b)(\\w+)\\1\\b'\n",
    "    \n",
    "    return re.sub(pattern, r'\\1 \\1', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kunjungan prabowo ini untuk meresmikan dan men...</td>\n",
       "      <td>Sumber Daya Alam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt anies dapat tepuk tangan meriah saat jadi r...</td>\n",
       "      <td>Politik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@ciqxqwgat04tmtx4ocatxjovq7vv/y8heyaiogmfg8y= ...</td>\n",
       "      <td>Demografi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt @l3r8xfbw3wgbxrpsj0/0hhztbqvgx7qtfwrg9zmhk7...</td>\n",
       "      <td>Politik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anies baswedan harap asn termasuk tni dan polr...</td>\n",
       "      <td>Politik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14559</th>\n",
       "      <td>Penggundulan hutan di Indonesia saat ini, menu...</td>\n",
       "      <td>Sumber Daya Alam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14560</th>\n",
       "      <td>Menurut Mahfud MD, Calon Wakil Presiden nomor ...</td>\n",
       "      <td>Sumber Daya Alam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14561</th>\n",
       "      <td>Menurut Calon Wakil Presiden nomor urut 3, Mah...</td>\n",
       "      <td>Sumber Daya Alam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14562</th>\n",
       "      <td>Calon Wakil Presiden nomor urut 3, Mahfud MD, ...</td>\n",
       "      <td>Sumber Daya Alam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14563</th>\n",
       "      <td>Mahfud MD, Calon Wakil Presiden nomor urut 3, ...</td>\n",
       "      <td>Sumber Daya Alam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14564 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text             label\n",
       "0      kunjungan prabowo ini untuk meresmikan dan men...  Sumber Daya Alam\n",
       "1      rt anies dapat tepuk tangan meriah saat jadi r...           Politik\n",
       "2      @ciqxqwgat04tmtx4ocatxjovq7vv/y8heyaiogmfg8y= ...         Demografi\n",
       "3      rt @l3r8xfbw3wgbxrpsj0/0hhztbqvgx7qtfwrg9zmhk7...           Politik\n",
       "4      anies baswedan harap asn termasuk tni dan polr...           Politik\n",
       "...                                                  ...               ...\n",
       "14559  Penggundulan hutan di Indonesia saat ini, menu...  Sumber Daya Alam\n",
       "14560  Menurut Mahfud MD, Calon Wakil Presiden nomor ...  Sumber Daya Alam\n",
       "14561  Menurut Calon Wakil Presiden nomor urut 3, Mah...  Sumber Daya Alam\n",
       "14562  Calon Wakil Presiden nomor urut 3, Mahfud MD, ...  Sumber Daya Alam\n",
       "14563  Mahfud MD, Calon Wakil Presiden nomor urut 3, ...  Sumber Daya Alam\n",
       "\n",
       "[14564 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"./dataset/processed dataset/DatasetTrainAllVersion.csv\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['clean_username'] = dataset['text'].apply(lambda x : clean_username(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['clean_text'] = dataset['clean_username'].apply(lambda x : clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['normalize_text'] = dataset['clean_text'].apply(lambda x : normalize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['clean_http'] = dataset['normalize_text'].apply(lambda x: re.sub(r'\\b\\w*https\\w*\\b', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_list = pd.read_csv(\"./dataset/corpus dataset/colloquial-indonesian-lexicon.csv\")\n",
    "slang = slang_list['slang'].values.tolist()\n",
    "formal = slang_list['formal'].values.tolist()\n",
    "slang_to_formal = dict(zip(slang, formal))\n",
    "\n",
    "dataset['formal'] = text_to_formal(dataset, slang_to_formal,'clean_http')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['preprocessed_formal'] = dataset['formal'].apply(separate_repeated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['preprocessed_formal_character'] = dataset['preprocessed_formal'].apply(remove_repeated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'label', 'clean_username', 'clean_text', 'normalize_text',\n",
       "       'clean_http', 'formal', 'preprocessed_formal',\n",
       "       'preprocessed_formal_character'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[['preprocessed_formal_character','label']].to_csv(\"all_version_cleaning.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NON TRANSFORMER PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_id.lemmatizer import Lemmatizer\n",
    "\n",
    "lemmatizer = Lemmatizer()\n",
    "lemmatized=[]\n",
    "for index, row in dataset.iterrows():\n",
    "    lemmatized.append(lemmatizer.lemmatize(row['formal']))\n",
    "\n",
    "dataset['lemmatized']=lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_id.stopword import StopWord\n",
    "\n",
    "stopword = StopWord()\n",
    "stopword_removed=[]\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    stopword_removed.append(stopword.remove_stopword(row['lemmatized']))\n",
    "\n",
    "dataset['stopword_removed']=stopword_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_id.tokenizer import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokens_c=[]\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    tokens = tokenizer.tokenize(row['stopword_removed'])\n",
    "    tokens_c.append(tokens)\n",
    "\n",
    "dataset['tokens']=tokens_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['tokens'] = dataset['tokens'].apply(lambda x : ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_username</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>normalize_text</th>\n",
       "      <th>clean_http</th>\n",
       "      <th>formal</th>\n",
       "      <th>preprocessed_formal</th>\n",
       "      <th>preprocessed_formal_character</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>stopword_removed</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kunjungan Prabowo ini untuk meresmikan dan men...</td>\n",
       "      <td>Sumber Daya Alam</td>\n",
       "      <td>Kunjungan Prabowo ini untuk meresmikan dan men...</td>\n",
       "      <td>Kunjungan Prabowo ini untuk meresmikan dan men...</td>\n",
       "      <td>kunjungan prabowo ini untuk meresmikan dan men...</td>\n",
       "      <td>kunjungan prabowo ini untuk meresmikan dan men...</td>\n",
       "      <td>kunjungan prabowo ini untuk meresmikan dan men...</td>\n",
       "      <td>kunjungan prabowo ini untuk meresmikan dan men...</td>\n",
       "      <td>kunjungan prabowo ini untuk meresmikan dan men...</td>\n",
       "      <td>kunjung prabowo ini untuk resmi dan serah proy...</td>\n",
       "      <td>kunjung prabowo resmi serah proyek bantu air b...</td>\n",
       "      <td>kunjung prabowo resmi serah proyek bantu air b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT Anies dapat tepuk tangan meriah saat jadi R...</td>\n",
       "      <td>Politik</td>\n",
       "      <td>RT Anies dapat tepuk tangan meriah saat jadi R...</td>\n",
       "      <td>Anies dapat tepuk tangan meriah saat jadi Rekt...</td>\n",
       "      <td>anies dapat tepuk tangan meriah saat jadi rekt...</td>\n",
       "      <td>anies dapat tepuk tangan meriah saat jadi rekt...</td>\n",
       "      <td>anies dapat tepuk tangan meriah saat jadi rekt...</td>\n",
       "      <td>anies dapat tepuk tangan meriah saat jadi rekt...</td>\n",
       "      <td>anies dapat tepuk tangan meriah sat jadi rekto...</td>\n",
       "      <td>anies dapat tepuk tangan riah saat jadi rektor...</td>\n",
       "      <td>anies tepuk tangan riah rektor wajib mata kuli...</td>\n",
       "      <td>anies tepuk tangan riah rektor wajib mata kuli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@CIqXqwGAT04tMtx4OCATxjoVq7vv/Y8HeYaIOgMFg8Y= ...</td>\n",
       "      <td>Demografi</td>\n",
       "      <td>emng bener sih, pendukung 01 ada yg goblok, b...</td>\n",
       "      <td>emng bener sih pendukung ada yg goblok begitu ...</td>\n",
       "      <td>emng bener sih pendukung ada yg goblok begitu ...</td>\n",
       "      <td>emng bener sih pendukung ada yg goblok begitu ...</td>\n",
       "      <td>memang benar sih pendukung ada yang goblok beg...</td>\n",
       "      <td>memang benar sih pendukung ada yang goblok beg...</td>\n",
       "      <td>memang benar sih pendukung ada yang goblok beg...</td>\n",
       "      <td>memang benar sih dukung ada yang goblok begitu...</td>\n",
       "      <td>dukung goblok dukung ridwan kamil skema kalo m...</td>\n",
       "      <td>dukung goblok dukung ridwan kamil skema kalo m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @L3R8XFBw3WGbxRPSj0/0hHZTbqVGX7qtfwRg9zmhK7...</td>\n",
       "      <td>Politik</td>\n",
       "      <td>RT  Sewaktu anies bersikap kritis ke kinerja p...</td>\n",
       "      <td>Sewaktu anies bersikap kritis ke kinerja pak p...</td>\n",
       "      <td>sewaktu anies bersikap kritis ke kinerja pak p...</td>\n",
       "      <td>sewaktu anies bersikap kritis ke kinerja pak p...</td>\n",
       "      <td>sewaktu anies bersikap kritis ke kinerja pak p...</td>\n",
       "      <td>sewaktu anies bersikap kritis ke kinerja pak p...</td>\n",
       "      <td>sewaktu anies bersikap kritis ke kinerja pak p...</td>\n",
       "      <td>waktu anies sikap kritis ke kerja pak prabowo ...</td>\n",
       "      <td>anies sikap kritis kerja prabowo anggap sopan ...</td>\n",
       "      <td>anies sikap kritis kerja prabowo anggap sopan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anies Baswedan Harap ASN termasuk TNI dan Polr...</td>\n",
       "      <td>Politik</td>\n",
       "      <td>Anies Baswedan Harap ASN termasuk TNI dan Polr...</td>\n",
       "      <td>Anies Baswedan Harap ASN termasuk TNI dan Polr...</td>\n",
       "      <td>anies baswedan harap asn termasuk tni dan polr...</td>\n",
       "      <td>anies baswedan harap asn termasuk tni dan polr...</td>\n",
       "      <td>anies baswedan harap aparatur sipil negara ter...</td>\n",
       "      <td>anies baswedan harap aparatur sipil negara ter...</td>\n",
       "      <td>anies baswedan harap aparatur sipil negara ter...</td>\n",
       "      <td>anies baswedan harap aparatur sipil negara mas...</td>\n",
       "      <td>anies baswedan harap aparatur sipil negara mas...</td>\n",
       "      <td>anies baswedan harap aparatur sipil negara mas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text             label  \\\n",
       "0  Kunjungan Prabowo ini untuk meresmikan dan men...  Sumber Daya Alam   \n",
       "1  RT Anies dapat tepuk tangan meriah saat jadi R...           Politik   \n",
       "2  @CIqXqwGAT04tMtx4OCATxjoVq7vv/Y8HeYaIOgMFg8Y= ...         Demografi   \n",
       "3  RT @L3R8XFBw3WGbxRPSj0/0hHZTbqVGX7qtfwRg9zmhK7...           Politik   \n",
       "4  Anies Baswedan Harap ASN termasuk TNI dan Polr...           Politik   \n",
       "\n",
       "                                      clean_username  \\\n",
       "0  Kunjungan Prabowo ini untuk meresmikan dan men...   \n",
       "1  RT Anies dapat tepuk tangan meriah saat jadi R...   \n",
       "2   emng bener sih, pendukung 01 ada yg goblok, b...   \n",
       "3  RT  Sewaktu anies bersikap kritis ke kinerja p...   \n",
       "4  Anies Baswedan Harap ASN termasuk TNI dan Polr...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  Kunjungan Prabowo ini untuk meresmikan dan men...   \n",
       "1  Anies dapat tepuk tangan meriah saat jadi Rekt...   \n",
       "2  emng bener sih pendukung ada yg goblok begitu ...   \n",
       "3  Sewaktu anies bersikap kritis ke kinerja pak p...   \n",
       "4  Anies Baswedan Harap ASN termasuk TNI dan Polr...   \n",
       "\n",
       "                                      normalize_text  \\\n",
       "0  kunjungan prabowo ini untuk meresmikan dan men...   \n",
       "1  anies dapat tepuk tangan meriah saat jadi rekt...   \n",
       "2  emng bener sih pendukung ada yg goblok begitu ...   \n",
       "3  sewaktu anies bersikap kritis ke kinerja pak p...   \n",
       "4  anies baswedan harap asn termasuk tni dan polr...   \n",
       "\n",
       "                                          clean_http  \\\n",
       "0  kunjungan prabowo ini untuk meresmikan dan men...   \n",
       "1  anies dapat tepuk tangan meriah saat jadi rekt...   \n",
       "2  emng bener sih pendukung ada yg goblok begitu ...   \n",
       "3  sewaktu anies bersikap kritis ke kinerja pak p...   \n",
       "4  anies baswedan harap asn termasuk tni dan polr...   \n",
       "\n",
       "                                              formal  \\\n",
       "0  kunjungan prabowo ini untuk meresmikan dan men...   \n",
       "1  anies dapat tepuk tangan meriah saat jadi rekt...   \n",
       "2  memang benar sih pendukung ada yang goblok beg...   \n",
       "3  sewaktu anies bersikap kritis ke kinerja pak p...   \n",
       "4  anies baswedan harap aparatur sipil negara ter...   \n",
       "\n",
       "                                 preprocessed_formal  \\\n",
       "0  kunjungan prabowo ini untuk meresmikan dan men...   \n",
       "1  anies dapat tepuk tangan meriah saat jadi rekt...   \n",
       "2  memang benar sih pendukung ada yang goblok beg...   \n",
       "3  sewaktu anies bersikap kritis ke kinerja pak p...   \n",
       "4  anies baswedan harap aparatur sipil negara ter...   \n",
       "\n",
       "                       preprocessed_formal_character  \\\n",
       "0  kunjungan prabowo ini untuk meresmikan dan men...   \n",
       "1  anies dapat tepuk tangan meriah sat jadi rekto...   \n",
       "2  memang benar sih pendukung ada yang goblok beg...   \n",
       "3  sewaktu anies bersikap kritis ke kinerja pak p...   \n",
       "4  anies baswedan harap aparatur sipil negara ter...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  kunjung prabowo ini untuk resmi dan serah proy...   \n",
       "1  anies dapat tepuk tangan riah saat jadi rektor...   \n",
       "2  memang benar sih dukung ada yang goblok begitu...   \n",
       "3  waktu anies sikap kritis ke kerja pak prabowo ...   \n",
       "4  anies baswedan harap aparatur sipil negara mas...   \n",
       "\n",
       "                                    stopword_removed  \\\n",
       "0  kunjung prabowo resmi serah proyek bantu air b...   \n",
       "1  anies tepuk tangan riah rektor wajib mata kuli...   \n",
       "2  dukung goblok dukung ridwan kamil skema kalo m...   \n",
       "3  anies sikap kritis kerja prabowo anggap sopan ...   \n",
       "4  anies baswedan harap aparatur sipil negara mas...   \n",
       "\n",
       "                                              tokens  \n",
       "0  kunjung prabowo resmi serah proyek bantu air b...  \n",
       "1  anies tepuk tangan riah rektor wajib mata kuli...  \n",
       "2  dukung goblok dukung ridwan kamil skema kalo m...  \n",
       "3  anies sikap kritis kerja prabowo anggap sopan ...  \n",
       "4  anies baswedan harap aparatur sipil negara mas...  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[['text','label']].to_csv(\"./training data/text-berttweet.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: './training data/bert-model.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[160], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mformal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./training data/bert-model.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\SatriaData24\\lib\\site-packages\\pandas\\core\\generic.py:3902\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3891\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3893\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3894\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3895\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3899\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3900\u001b[0m )\n\u001b[1;32m-> 3902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3905\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3907\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\SatriaData24\\lib\\site-packages\\pandas\\io\\formats\\format.py:1152\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1134\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1135\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1151\u001b[0m )\n\u001b[1;32m-> 1152\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1155\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\SatriaData24\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:247\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    257\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    258\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    264\u001b[0m     )\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\SatriaData24\\lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: './training data/bert-model.csv'"
     ]
    }
   ],
   "source": [
    "dataset[['formal','label']].to_csv(\"./training data/bert-model.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[['tokens','label']].to_csv(\"./training data/tokens_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INDOBERTWEET PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDText</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TXT0001</td>\n",
       "      <td>Lu mau org2 pro-demokrasi di negara ini bisa p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TXT0002</td>\n",
       "      <td>Prabowo ditanya soal hutang luar negeri dia me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TXT0003</td>\n",
       "      <td>kiki_daliyo  Ganjar Pranowo itulah beliau soso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TXT0004</td>\n",
       "      <td>@kumparan Prabowo Gibran yang bisa melakukan i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TXT0005</td>\n",
       "      <td>@sniperruben45 @uda_zulhendra @ainunnajib Lah ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>TXT0996</td>\n",
       "      <td>Bikin bangga deh, Ganjar-Mahfud mau alokasikan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>TXT0997</td>\n",
       "      <td>Pak Jokowi sebelum pilpres 2024 berbesar hati ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>TXT0998</td>\n",
       "      <td>@datuakrajoangek Sbaiknya si gemot nga usah ik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>TXT0999</td>\n",
       "      <td>kebiasaan merembuk atau bermusyawarah jadi gay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>TXT1000</td>\n",
       "      <td>Mirage Ditolak Juwono, Dibeli Prabowo, Jubir T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      IDText                                               text\n",
       "0    TXT0001  Lu mau org2 pro-demokrasi di negara ini bisa p...\n",
       "1    TXT0002  Prabowo ditanya soal hutang luar negeri dia me...\n",
       "2    TXT0003  kiki_daliyo  Ganjar Pranowo itulah beliau soso...\n",
       "3    TXT0004  @kumparan Prabowo Gibran yang bisa melakukan i...\n",
       "4    TXT0005  @sniperruben45 @uda_zulhendra @ainunnajib Lah ...\n",
       "..       ...                                                ...\n",
       "995  TXT0996  Bikin bangga deh, Ganjar-Mahfud mau alokasikan...\n",
       "996  TXT0997  Pak Jokowi sebelum pilpres 2024 berbesar hati ...\n",
       "997  TXT0998  @datuakrajoangek Sbaiknya si gemot nga usah ik...\n",
       "998  TXT0999  kebiasaan merembuk atau bermusyawarah jadi gay...\n",
       "999  TXT1000  Mirage Ditolak Juwono, Dibeli Prabowo, Jubir T...\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_bert = pd.read_csv(\"./dataset/raw dataset/dataset_unlabeled_penyisihan_bdc_2024.csv\",delimiter=\";\")\n",
    "twitter_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_bert.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDText</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TXT0001</td>\n",
       "      <td>Lu mau org2 pro-demokrasi di negara ini bisa p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TXT0002</td>\n",
       "      <td>Prabowo ditanya soal hutang luar negeri dia me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TXT0003</td>\n",
       "      <td>kiki_daliyo  Ganjar Pranowo itulah beliau soso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TXT0004</td>\n",
       "      <td>@kumparan Prabowo Gibran yang bisa melakukan i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TXT0005</td>\n",
       "      <td>@sniperruben45 @uda_zulhendra @ainunnajib Lah ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>TXT0996</td>\n",
       "      <td>Bikin bangga deh, Ganjar-Mahfud mau alokasikan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>TXT0997</td>\n",
       "      <td>Pak Jokowi sebelum pilpres 2024 berbesar hati ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>TXT0998</td>\n",
       "      <td>@datuakrajoangek Sbaiknya si gemot nga usah ik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>TXT0999</td>\n",
       "      <td>kebiasaan merembuk atau bermusyawarah jadi gay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>TXT1000</td>\n",
       "      <td>Mirage Ditolak Juwono, Dibeli Prabowo, Jubir T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      IDText                                               text\n",
       "0    TXT0001  Lu mau org2 pro-demokrasi di negara ini bisa p...\n",
       "1    TXT0002  Prabowo ditanya soal hutang luar negeri dia me...\n",
       "2    TXT0003  kiki_daliyo  Ganjar Pranowo itulah beliau soso...\n",
       "3    TXT0004  @kumparan Prabowo Gibran yang bisa melakukan i...\n",
       "4    TXT0005  @sniperruben45 @uda_zulhendra @ainunnajib Lah ...\n",
       "..       ...                                                ...\n",
       "995  TXT0996  Bikin bangga deh, Ganjar-Mahfud mau alokasikan...\n",
       "996  TXT0997  Pak Jokowi sebelum pilpres 2024 berbesar hati ...\n",
       "997  TXT0998  @datuakrajoangek Sbaiknya si gemot nga usah ik...\n",
       "998  TXT0999  kebiasaan merembuk atau bermusyawarah jadi gay...\n",
       "999  TXT1000  Mirage Ditolak Juwono, Dibeli Prabowo, Jubir T...\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Politik                    3094\n",
       "Ideologi                    603\n",
       "Pertahanan dan Keamanan     546\n",
       "Sosial Budaya               515\n",
       "Ekonomi                     479\n",
       "Sumber Daya Alam            409\n",
       "Demografi                   401\n",
       "Geografi                    385\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_bert['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emoji import demojize\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # Translate emojis to text\n",
    "    tweet = demojize(tweet)\n",
    "    \n",
    "    # Convert user mentions and URLs to special tokens\n",
    "    tweet = re.sub(r'[@#]\\S+', '@USER', tweet)\n",
    "    tweet = re.sub(r'http\\S+|www\\S+', 'HTTPURL', tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_non_alphabetic(text):\n",
    "    text = re.sub(r'\\brt\\b', '', text)  # Remove 'RT'\n",
    "    text = re.sub(r'\\[re [^\\]]+\\]', '', text)  # Remove '[RE xxxx]'\n",
    "    # Preserve @USER while removing non-alphabetic characters\n",
    "    text = re.sub(r'(@USER)|[^a-zA-Z\\s]', lambda m: m.group(1) if m.group(1) else '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_bert['text'] = twitter_bert['text'].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_bert['preprocessed_text'] = twitter_bert['text'].apply(lambda x : preprocess_tweet(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_bert['preprocessed_text_two'] = twitter_bert['preprocessed_text'].apply(lambda x : clean_non_alphabetic(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_bert['formal'] = text_to_formal(twitter_bert, slang_to_formal,'preprocessed_text_two')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_bert['formal'] = twitter_bert['formal'].apply(lambda x : separate_repeated_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_bert_dua =  twitter_bert.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDText</th>\n",
       "      <th>text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>preprocessed_text_two</th>\n",
       "      <th>formal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TXT0001</td>\n",
       "      <td>lu mau org2 pro-demokrasi di negara ini bisa p...</td>\n",
       "      <td>lu mau org2 pro-demokrasi di negara ini bisa p...</td>\n",
       "      <td>lu mau org prodemokrasi di negara ini bisa pun...</td>\n",
       "      <td>lu mau orang prodemokrasi di negara ini bisa p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TXT0002</td>\n",
       "      <td>prabowo ditanya soal hutang luar negeri dia me...</td>\n",
       "      <td>prabowo ditanya soal hutang luar negeri dia me...</td>\n",
       "      <td>prabowo ditanya soal hutang luar negeri dia me...</td>\n",
       "      <td>prabowo ditanya soal hutang luar negeri dia me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TXT0003</td>\n",
       "      <td>kiki_daliyo  ganjar pranowo itulah beliau soso...</td>\n",
       "      <td>kiki_daliyo  ganjar pranowo itulah beliau soso...</td>\n",
       "      <td>kikidaliyo ganjar pranowo itulah beliau sosok ...</td>\n",
       "      <td>kikidaliyo ganjar pranowo itulah beliau sosok ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TXT0004</td>\n",
       "      <td>@kumparan prabowo gibran yang bisa melakukan i...</td>\n",
       "      <td>@USER prabowo gibran yang bisa melakukan itu s...</td>\n",
       "      <td>@USER prabowo gibran yang bisa melakukan itu s...</td>\n",
       "      <td>@USER prabowo gibran yang bisa melakukan itu s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TXT0005</td>\n",
       "      <td>@sniperruben45 @uda_zulhendra @ainunnajib lah ...</td>\n",
       "      <td>@USER @USER @USER lah justru yg gak nyambung j...</td>\n",
       "      <td>@USER @USER @USER lah justru yg gak nyambung j...</td>\n",
       "      <td>@USER @USER @USER lah justru yang tidak menyam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>TXT0996</td>\n",
       "      <td>bikin bangga deh, ganjar-mahfud mau alokasikan...</td>\n",
       "      <td>bikin bangga deh, ganjar-mahfud mau alokasikan...</td>\n",
       "      <td>bikin bangga deh ganjarmahfud mau alokasikan s...</td>\n",
       "      <td>bikin bangga deh ganjar mahfud mau alokasikan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>TXT0997</td>\n",
       "      <td>pak jokowi sebelum pilpres 2024 berbesar hati ...</td>\n",
       "      <td>pak jokowi sebelum pilpres 2024 berbesar hati ...</td>\n",
       "      <td>pak jokowi sebelum pilpres berbesar hati meran...</td>\n",
       "      <td>pak jokowi sebelum pemilihan presiden berbesar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>TXT0998</td>\n",
       "      <td>@datuakrajoangek sbaiknya si gemot nga usah ik...</td>\n",
       "      <td>@USER sbaiknya si gemot nga usah ikutan debat ...</td>\n",
       "      <td>@USER sbaiknya si gemot nga usah ikutan debat ...</td>\n",
       "      <td>@USER sebaiknya sih gemot tidak usah ikutan de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>TXT0999</td>\n",
       "      <td>kebiasaan merembuk atau bermusyawarah jadi gay...</td>\n",
       "      <td>kebiasaan merembuk atau bermusyawarah jadi gay...</td>\n",
       "      <td>kebiasaan merembuk atau bermusyawarah jadi gay...</td>\n",
       "      <td>kebiasaan merembuk atau bermusyawarah jadi gay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>TXT1000</td>\n",
       "      <td>mirage ditolak juwono, dibeli prabowo, jubir t...</td>\n",
       "      <td>mirage ditolak juwono, dibeli prabowo, jubir t...</td>\n",
       "      <td>mirage ditolak juwono dibeli prabowo jubir tim...</td>\n",
       "      <td>mirage ditolak juwono dibeli prabowo jubir tim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      IDText                                               text  \\\n",
       "0    TXT0001  lu mau org2 pro-demokrasi di negara ini bisa p...   \n",
       "1    TXT0002  prabowo ditanya soal hutang luar negeri dia me...   \n",
       "2    TXT0003  kiki_daliyo  ganjar pranowo itulah beliau soso...   \n",
       "3    TXT0004  @kumparan prabowo gibran yang bisa melakukan i...   \n",
       "4    TXT0005  @sniperruben45 @uda_zulhendra @ainunnajib lah ...   \n",
       "..       ...                                                ...   \n",
       "995  TXT0996  bikin bangga deh, ganjar-mahfud mau alokasikan...   \n",
       "996  TXT0997  pak jokowi sebelum pilpres 2024 berbesar hati ...   \n",
       "997  TXT0998  @datuakrajoangek sbaiknya si gemot nga usah ik...   \n",
       "998  TXT0999  kebiasaan merembuk atau bermusyawarah jadi gay...   \n",
       "999  TXT1000  mirage ditolak juwono, dibeli prabowo, jubir t...   \n",
       "\n",
       "                                     preprocessed_text  \\\n",
       "0    lu mau org2 pro-demokrasi di negara ini bisa p...   \n",
       "1    prabowo ditanya soal hutang luar negeri dia me...   \n",
       "2    kiki_daliyo  ganjar pranowo itulah beliau soso...   \n",
       "3    @USER prabowo gibran yang bisa melakukan itu s...   \n",
       "4    @USER @USER @USER lah justru yg gak nyambung j...   \n",
       "..                                                 ...   \n",
       "995  bikin bangga deh, ganjar-mahfud mau alokasikan...   \n",
       "996  pak jokowi sebelum pilpres 2024 berbesar hati ...   \n",
       "997  @USER sbaiknya si gemot nga usah ikutan debat ...   \n",
       "998  kebiasaan merembuk atau bermusyawarah jadi gay...   \n",
       "999  mirage ditolak juwono, dibeli prabowo, jubir t...   \n",
       "\n",
       "                                 preprocessed_text_two  \\\n",
       "0    lu mau org prodemokrasi di negara ini bisa pun...   \n",
       "1    prabowo ditanya soal hutang luar negeri dia me...   \n",
       "2    kikidaliyo ganjar pranowo itulah beliau sosok ...   \n",
       "3    @USER prabowo gibran yang bisa melakukan itu s...   \n",
       "4    @USER @USER @USER lah justru yg gak nyambung j...   \n",
       "..                                                 ...   \n",
       "995  bikin bangga deh ganjarmahfud mau alokasikan s...   \n",
       "996  pak jokowi sebelum pilpres berbesar hati meran...   \n",
       "997  @USER sbaiknya si gemot nga usah ikutan debat ...   \n",
       "998  kebiasaan merembuk atau bermusyawarah jadi gay...   \n",
       "999  mirage ditolak juwono dibeli prabowo jubir tim...   \n",
       "\n",
       "                                                formal  \n",
       "0    lu mau orang prodemokrasi di negara ini bisa p...  \n",
       "1    prabowo ditanya soal hutang luar negeri dia me...  \n",
       "2    kikidaliyo ganjar pranowo itulah beliau sosok ...  \n",
       "3    @USER prabowo gibran yang bisa melakukan itu s...  \n",
       "4    @USER @USER @USER lah justru yang tidak menyam...  \n",
       "..                                                 ...  \n",
       "995  bikin bangga deh ganjar mahfud mau alokasikan ...  \n",
       "996  pak jokowi sebelum pemilihan presiden berbesar...  \n",
       "997  @USER sebaiknya sih gemot tidak usah ikutan de...  \n",
       "998  kebiasaan merembuk atau bermusyawarah jadi gay...  \n",
       "999  mirage ditolak juwono dibeli prabowo jubir tim...  \n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "formal_predict = twitter_bert[['IDText','formal']].to_csv(\"./predict data/formal.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = twitter_bert[['IDText','text']].to_csv(\"./predict data/raw_text.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_bert_dua.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Politik                    3094\n",
       "Ideologi                    603\n",
       "Pertahanan dan Keamanan     546\n",
       "Sosial Budaya               515\n",
       "Ekonomi                     479\n",
       "Sumber Daya Alam            409\n",
       "Demografi                   401\n",
       "Geografi                    385\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_bert_dua['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_bert_dua[['formal','label']].dropna().to_csv(\"indotweet_train.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANSLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_5736\\2183237927.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  twitter_bert['english'] =  twitter_bert['formal'].apply(lambda x : translate_text(x, src='id', dest='en'))\n"
     ]
    }
   ],
   "source": [
    "# from googletrans import Translator\n",
    "# import json\n",
    "\n",
    "# # Initialize the translator\n",
    "# translator = Translator()\n",
    "\n",
    "# # Function to translate text\n",
    "# def translate_text(text, src='auto', dest='en'):\n",
    "#     try:\n",
    "#         translated = translator.translate(text, src=src, dest=dest)\n",
    "#         return translated.text\n",
    "#     except json.decoder.JSONDecodeError as json_err:\n",
    "#         print(f\"JSONDecodeError occurred: {json_err}\")\n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "#         return None\n",
    "\n",
    "# twitter_bert['english'] =  twitter_bert['formal'].apply(lambda x : translate_text(x, src='id', dest='en'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESS ENGLISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_5736\\3594764977.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  twitter_bert['english_processed'] = twitter_bert['english'].apply(lambda x :clean_username(x))\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_5736\\3594764977.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  twitter_bert['english_processed'] = twitter_bert['english_processed'].apply(lambda x :normalize_text(x))\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_5736\\3594764977.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  twitter_bert['english_processed'] = twitter_bert['english_processed'].apply(lambda x :re.sub(r'\\b\\w*htt\\w*\\b', '', x))\n"
     ]
    }
   ],
   "source": [
    "# twitter_bert['english_processed'] = twitter_bert['english'].apply(lambda x :clean_username(x))\n",
    "# twitter_bert['english_processed'] = twitter_bert['english_processed'].apply(lambda x :normalize_text(x))\n",
    "# twitter_bert['english_processed'] = twitter_bert['english_processed'].apply(lambda x :re.sub(r'\\b\\w*htt\\w*\\b', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_bert[['english_processed','label']].dropna().to_csv(\"english_process.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRY AUGMENTED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_processed</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>prabowos visit was to inaugurate and submit a ...</td>\n",
       "      <td>Sumber Daya Alam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anies can be applauded when becoming the chanc...</td>\n",
       "      <td>Politik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is true that there are supporters who are s...</td>\n",
       "      <td>Demografi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when anies was critical of pak prabowos perfor...</td>\n",
       "      <td>Politik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anies baswedan hopes that the state civil appa...</td>\n",
       "      <td>Politik</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   english_processed             label\n",
       "0  prabowos visit was to inaugurate and submit a ...  Sumber Daya Alam\n",
       "1  anies can be applauded when becoming the chanc...           Politik\n",
       "2  it is true that there are supporters who are s...         Demografi\n",
       "3  when anies was critical of pak prabowos perfor...           Politik\n",
       "4  anies baswedan hopes that the state civil appa...           Politik"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_data = pd.read_csv(\"./english_process.csv\")\n",
    "\n",
    "english_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "\n",
    "from nlpaug.util import Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_processed</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>prabowos visit was to inaugurate and submit a ...</td>\n",
       "      <td>Sumber Daya Alam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anies can be applauded when becoming the chanc...</td>\n",
       "      <td>Politik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is true that there are supporters who are s...</td>\n",
       "      <td>Demografi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when anies was critical of pak prabowos perfor...</td>\n",
       "      <td>Politik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anies baswedan hopes that the state civil appa...</td>\n",
       "      <td>Politik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>seeing the debate yesterday when prabowo kicep...</td>\n",
       "      <td>Politik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>the community believes that prabowo gibran has...</td>\n",
       "      <td>Politik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>imo both are irrational but one is much more i...</td>\n",
       "      <td>Ekonomi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>look at that mr ganjar you have been involved ...</td>\n",
       "      <td>Pertahanan dan Keamanan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>this event not only cooks but the presidential...</td>\n",
       "      <td>Sumber Daya Alam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4990 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      english_processed  \\\n",
       "0     prabowos visit was to inaugurate and submit a ...   \n",
       "1     anies can be applauded when becoming the chanc...   \n",
       "2     it is true that there are supporters who are s...   \n",
       "3     when anies was critical of pak prabowos perfor...   \n",
       "4     anies baswedan hopes that the state civil appa...   \n",
       "...                                                 ...   \n",
       "4995  seeing the debate yesterday when prabowo kicep...   \n",
       "4996  the community believes that prabowo gibran has...   \n",
       "4997  imo both are irrational but one is much more i...   \n",
       "4998  look at that mr ganjar you have been involved ...   \n",
       "4999  this event not only cooks but the presidential...   \n",
       "\n",
       "                        label  \n",
       "0            Sumber Daya Alam  \n",
       "1                     Politik  \n",
       "2                   Demografi  \n",
       "3                     Politik  \n",
       "4                     Politik  \n",
       "...                       ...  \n",
       "4995                  Politik  \n",
       "4996                  Politik  \n",
       "4997                  Ekonomi  \n",
       "4998  Pertahanan dan Keamanan  \n",
       "4999         Sumber Daya Alam  \n",
       "\n",
       "[4990 rows x 2 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m aug \u001b[38;5;241m=\u001b[39m naw\u001b[38;5;241m.\u001b[39mContextualWordEmbsAug(\n\u001b[0;32m      2\u001b[0m     model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m, action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubstitute\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# augmented_text = aug.augment(text)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# print(\"Original:\")\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# print(text)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# print(\"Augmented Text:\")\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m english_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maugmented_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43menglish_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menglish_processed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maug\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\SatriaData24\\lib\\site-packages\\pandas\\core\\series.py:4757\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4640\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4762\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4764\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\SatriaData24\\lib\\site-packages\\pandas\\core\\apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\SatriaData24\\lib\\site-packages\\pandas\\core\\apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\SatriaData24\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\SatriaData24\\lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[118], line 7\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m aug \u001b[38;5;241m=\u001b[39m naw\u001b[38;5;241m.\u001b[39mContextualWordEmbsAug(\n\u001b[0;32m      2\u001b[0m     model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m, action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubstitute\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# augmented_text = aug.augment(text)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# print(\"Original:\")\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# print(text)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# print(\"Augmented Text:\")\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m english_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maugmented_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m english_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish_processed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43maug\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\SatriaData24\\lib\\site-packages\\nlpaug\\base_augmenter.py:57\u001b[0m, in \u001b[0;36mAugmenter.augment\u001b[1;34m(self, data, n, num_thread)\u001b[0m\n\u001b[0;32m     54\u001b[0m aug_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m n\n\u001b[0;32m     55\u001b[0m expected_output_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m aug_num\n\u001b[1;32m---> 57\u001b[0m exceptions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_augment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# TODO: Handle multiple exceptions\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m exception \u001b[38;5;129;01min\u001b[39;00m exceptions:\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\SatriaData24\\lib\\site-packages\\nlpaug\\base_augmenter.py:175\u001b[0m, in \u001b[0;36mAugmenter._validate_augment\u001b[1;34m(cls, data)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_augment\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data):\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [WarningException(name\u001b[38;5;241m=\u001b[39mWarningName\u001b[38;5;241m.\u001b[39mINPUT_VALIDATION_WARNING,\n\u001b[0;32m    177\u001b[0m                                  code\u001b[38;5;241m=\u001b[39mWarningCode\u001b[38;5;241m.\u001b[39mWARNING_CODE_001, msg\u001b[38;5;241m=\u001b[39mWarningMessage\u001b[38;5;241m.\u001b[39mLENGTH_IS_ZERO)]\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-uncased', action=\"substitute\")\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "english_data['augmented_text'] = english_data['english_processed'].apply(lambda x: aug.augment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "back_translation_aug = naw.BackTranslationAug(\n",
    "    from_model_name='facebook/wmt19-en-de', \n",
    "    to_model_name='facebook/wmt19-de-en'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Companies can applaud when it comes to running anti-corruption courses as chancellor to break the chain of corruption']\n"
     ]
    }
   ],
   "source": [
    "augmented_text = back_translation_aug.augment(text)\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "anies can be applauded when becoming the chancellor requires anti corruption courses to break the chain of corruption  \n",
      "Augmented Text:\n",
      "['anies can stay consulted when speaking the chief requires anti corruption courses should follow the chain of governance']\n"
     ]
    }
   ],
   "source": [
    "aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='distilbert-base-uncased', action=\"substitute\")\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING AUGMENTED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_processed</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>prabowos visit was to inaugurate and submit a ...</td>\n",
       "      <td>Sumber Daya Alam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anies can be applauded when becoming the chanc...</td>\n",
       "      <td>Politik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is true that there are supporters who are s...</td>\n",
       "      <td>Demografi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when anies was critical of pak prabowos perfor...</td>\n",
       "      <td>Politik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anies baswedan hopes that the state civil appa...</td>\n",
       "      <td>Politik</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   english_processed             label\n",
       "0  prabowos visit was to inaugurate and submit a ...  Sumber Daya Alam\n",
       "1  anies can be applauded when becoming the chanc...           Politik\n",
       "2  it is true that there are supporters who are s...         Demografi\n",
       "3  when anies was critical of pak prabowos perfor...           Politik\n",
       "4  anies baswedan hopes that the state civil appa...           Politik"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_augmented = pd.read_csv(\"./training data/augmented_data.csv\")\n",
    "\n",
    "data_augmented.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmented['english_processed_two'] = data_augmented['english_processed'].apply(lambda x : normalize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmented[['english_processed_two','label']].to_csv(\"augmented_training.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SatriaData24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
